{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dd\\.virtualenvs\\tf_rec-5MMI-Cvb\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"retrival\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"hdfs://localhost:9000/user/nhom7/book/data/\"\n",
    "bookFilePath = dataPath + \"BX-Books.csv\"\n",
    "ratingsFilePath = dataPath + \"BX-Book-Ratings.csv\"\n",
    "books_df = spark.read.options(inferSchema=\"true\", header=\"true\", delimiter=';').csv(bookFilePath)\n",
    "rating_df = spark.read.options(inferSchema=\"true\", header=\"true\", delimiter=';').csv(ratingsFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_pd = rating_df.drop('Book-Rating').toPandas()\n",
    "book_pd = books_df.select('ISBN').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tf.data.Dataset.from_tensor_slices(dict(rating_pd))\n",
    "books = tf.data.Dataset.from_tensor_slices(dict(book_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dd\\.virtualenvs\\tf_rec-5MMI-Cvb\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"isbn\": x[\"ISBN\"],\n",
    "    \"user_id\": x[\"User-ID\"],\n",
    "})\n",
    "\n",
    "books = books.map(lambda x: x[\"ISBN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ISBN = books.batch(1_000)\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_book_ISBN = np.unique(np.concatenate(list(book_ISBN)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.IntegerLookup(\n",
    "      vocabulary=unique_user_ids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_book_ISBN, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_book_ISBN) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=books.batch(128).map(book_model)\n",
    ")\n",
    "\n",
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrivalModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_model, book_model):\n",
    "    super().__init__()\n",
    "    self.book_model: tf.keras.Model = book_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    # And pick out the movie features and pass them into the movie model,\n",
    "    # getting embeddings back.\n",
    "    positive_book_embeddings = self.book_model(features[\"isbn\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_book_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RetrivalModel(user_model, book_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 1525s 151s/step - factorized_top_k/top_1_categorical_accuracy: 0.0632 - factorized_top_k/top_5_categorical_accuracy: 0.0652 - factorized_top_k/top_10_categorical_accuracy: 0.0657 - factorized_top_k/top_50_categorical_accuracy: 0.0685 - factorized_top_k/top_100_categorical_accuracy: 0.0704 - loss: 69961.7251 - regularization_loss: 0.0000e+00 - total_loss: 69961.7251  \n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 1397s 140s/step - factorized_top_k/top_1_categorical_accuracy: 0.0888 - factorized_top_k/top_5_categorical_accuracy: 0.0923 - factorized_top_k/top_10_categorical_accuracy: 0.0946 - factorized_top_k/top_50_categorical_accuracy: 0.1044 - factorized_top_k/top_100_categorical_accuracy: 0.1122 - loss: 68725.9830 - regularization_loss: 0.0000e+00 - total_loss: 68725.9830\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 1294s 129s/step - factorized_top_k/top_1_categorical_accuracy: 0.0736 - factorized_top_k/top_5_categorical_accuracy: 0.0906 - factorized_top_k/top_10_categorical_accuracy: 0.0957 - factorized_top_k/top_50_categorical_accuracy: 0.1162 - factorized_top_k/top_100_categorical_accuracy: 0.1316 - loss: 65681.7727 - regularization_loss: 0.0000e+00 - total_loss: 65681.7727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25edabde770>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 361s 74s/step - factorized_top_k/top_1_categorical_accuracy: 0.0240 - factorized_top_k/top_5_categorical_accuracy: 0.0415 - factorized_top_k/top_10_categorical_accuracy: 0.0443 - factorized_top_k/top_50_categorical_accuracy: 0.0474 - factorized_top_k/top_100_categorical_accuracy: 0.0507 - loss: 32570.6862 - regularization_loss: 0.0000e+00 - total_loss: 32570.6862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.024000000208616257,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.041450001299381256,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.0442500002682209,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.04740000143647194,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.05065000057220459,\n",
       " 'loss': 29616.52734375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 29616.52734375}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'0345404793' b'0380841940' b'0451129040' b'0812510488' b'0553114271'\n",
      " b'1551668459' b'0425158632' b'0345308808' b'0373087187' b'0375724958'\n",
      " b'0312952066' b'0936672765' b'037325640X' b'0373271069' b'0345308921'\n",
      " b'0517708957' b'0441800106' b'0373168861' b'1854879995' b'0380807351'], shape=(20,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "bruteforce = tfrs.layers.factorized_top_k.BruteForce(model.user_model, k=20)\n",
    "# recommends movies out of the entire movies dataset.\n",
    "bruteforce.index_from_dataset(\n",
    "  tf.data.Dataset.zip((books.batch(128), books.batch(128).map(model.book_model)))\n",
    ")\n",
    "\n",
    "# Get recommendations.\n",
    "_, titles = bruteforce(tf.constant([12]))\n",
    "# \n",
    "\n",
    "for t in titles:\n",
    "  print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/retrival_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/retrival_model/1/assets\n"
     ]
    }
   ],
   "source": [
    "path = (\"./model/retrival_model/1/\")\n",
    "tf.saved_model.save(\n",
    "    bruteforce,\n",
    "    path\n",
    ")\n",
    "\n",
    "loaded = tf.saved_model.load(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_rec-5MMI-Cvb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77e106df7f450c66566a7610a3718f206c95a104637392e72bdeaab378155cd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
